import { logger } from '@/lib/services';
/**\n * Performance Alerting Service\n * \n * Provides comprehensive alerting capabilities for performance monitoring,\n * including rule-based alerts, escalation, notifications, and alert management.\n */\n\nimport { supabase } from '@/lib/supabase';\nimport { enhancedPerformanceMonitor, type PerformanceAlert, type AlertSeverity } from '@/lib/utils/enhanced-performance-monitor';\n\n// =====================================================================================\n// TYPES AND INTERFACES\n// =====================================================================================\n\nexport interface AlertRule {\n  id: string;\n  name: string;\n  description: string;\n  category: 'performance' | 'availability' | 'error_rate' | 'user_experience' | 'resource_usage';\n  \n  // Rule conditions\n  metric: string; // e.g., 'api.average', 'web_vitals.LCP', 'error_rate'\n  operator: 'gt' | 'lt' | 'gte' | 'lte' | 'eq' | 'ne';\n  threshold: number;\n  duration: number; // How long condition must persist (ms)\n  \n  // Alert settings\n  severity: AlertSeverity;\n  enabled: boolean;\n  autoResolve: boolean;\n  resolutionThreshold?: number;\n  \n  // Notification settings\n  notificationChannels: NotificationChannel[];\n  escalationRules?: EscalationRule[];\n  \n  // Advanced settings\n  evaluationWindow: number; // Time window for evaluation (ms)\n  suppressionWindow: number; // Minimum time between similar alerts (ms)\n  tags: string[];\n  \n  // Metadata\n  createdAt: string;\n  updatedAt: string;\n  createdBy: string;\n}\n\nexport interface NotificationChannel {\n  type: 'email' | 'sms' | 'slack' | 'webhook' | 'in_app';\n  target: string; // email address, phone number, webhook URL, etc.\n  enabled: boolean;\n  settings?: Record<string, any>;\n}\n\nexport interface EscalationRule {\n  delay: number; // Escalation delay in ms\n  severity: AlertSeverity;\n  channels: NotificationChannel[];\n  condition?: {\n    metric: string;\n    operator: string;\n    threshold: number;\n  };\n}\n\nexport interface AlertEvent {\n  id: string;\n  ruleId: string;\n  alertId?: string;\n  \n  // Event details\n  type: 'triggered' | 'resolved' | 'escalated' | 'acknowledged' | 'suppressed';\n  timestamp: string;\n  \n  // Context\n  value: number;\n  threshold: number;\n  metadata?: Record<string, any>;\n  \n  // User actions\n  userId?: string;\n  reason?: string;\n  notes?: string;\n}\n\nexport interface AlertStats {\n  totalAlerts: number;\n  activeAlerts: number;\n  alertsByCategory: Record<string, number>;\n  alertsBySeverity: Record<AlertSeverity, number>;\n  averageResolutionTime: number;\n  falsePositiveRate: number;\n  escalationRate: number;\n  topAlertRules: Array<{\n    ruleId: string;\n    ruleName: string;\n    count: number;\n    averageResolutionTime: number;\n  }>;\n}\n\nexport interface AlertNotification {\n  id: string;\n  alertId: string;\n  ruleId: string;\n  channel: NotificationChannel;\n  \n  // Delivery status\n  status: 'pending' | 'sent' | 'delivered' | 'failed' | 'bounced';\n  sentAt?: string;\n  deliveredAt?: string;\n  failureReason?: string;\n  \n  // Retry logic\n  retryCount: number;\n  nextRetryAt?: string;\n  maxRetries: number;\n  \n  // Content\n  subject: string;\n  message: string;\n  metadata?: Record<string, any>;\n}\n\n// =====================================================================================\n// ALERT RULE ENGINE\n// =====================================================================================\n\nclass AlertRuleEngine {\n  private rules: Map<string, AlertRule> = new Map();\n  private activeAlerts: Map<string, PerformanceAlert> = new Map();\n  private suppressedAlerts: Set<string> = new Set();\n  private lastEvaluations: Map<string, number> = new Map();\n  \n  constructor() {\n    this.loadRules();\n    this.startEvaluationLoop();\n  }\n  \n  /**\n   * Load alert rules from database\n   */\n  private async loadRules() {\n    try {\n      const { data: rules, error } = await supabase\n        .from('performance_alert_rules')\n        .select('*')\n        .eq('enabled', true);\n      \n      if (error) throw error;\n      \n      rules?.forEach(rule => {\n        this.rules.set(rule.id, rule);\n      });\n      \n      logger.info(`Loaded ${rules?.length || 0} alert rules`);\n    } catch (error) {\n      logger.error('Failed to load alert rules:', error);\n    }\n  }\n  \n  /**\n   * Add or update an alert rule\n   */\n  async addRule(rule: Omit<AlertRule, 'id' | 'createdAt' | 'updatedAt'>): Promise<string> {\n    const ruleWithId: AlertRule = {\n      ...rule,\n      id: this.generateId(),\n      createdAt: new Date().toISOString(),\n      updatedAt: new Date().toISOString()\n    };\n    \n    try {\n      const { error } = await supabase\n        .from('performance_alert_rules')\n        .insert(ruleWithId);\n      \n      if (error) throw error;\n      \n      this.rules.set(ruleWithId.id, ruleWithId);\n      logger.info(`Added alert rule: ${rule.name}`);\n      \n      return ruleWithId.id;\n    } catch (error) {\n      logger.error('Failed to add alert rule:', error);\n      throw error;\n    }\n  }\n  \n  /**\n   * Update an existing alert rule\n   */\n  async updateRule(ruleId: string, updates: Partial<AlertRule>): Promise<void> {\n    const existingRule = this.rules.get(ruleId);\n    if (!existingRule) {\n      throw new Error(`Alert rule ${ruleId} not found`);\n    }\n    \n    const updatedRule: AlertRule = {\n      ...existingRule,\n      ...updates,\n      updatedAt: new Date().toISOString()\n    };\n    \n    try {\n      const { error } = await supabase\n        .from('performance_alert_rules')\n        .update(updatedRule)\n        .eq('id', ruleId);\n      \n      if (error) throw error;\n      \n      this.rules.set(ruleId, updatedRule);\n      logger.info(`Updated alert rule: ${updatedRule.name}`);\n    } catch (error) {\n      logger.error('Failed to update alert rule:', error);\n      throw error;\n    }\n  }\n  \n  /**\n   * Delete an alert rule\n   */\n  async deleteRule(ruleId: string): Promise<void> {\n    try {\n      const { error } = await supabase\n        .from('performance_alert_rules')\n        .delete()\n        .eq('id', ruleId);\n      \n      if (error) throw error;\n      \n      this.rules.delete(ruleId);\n      logger.info(`Deleted alert rule: ${ruleId}`);\n    } catch (error) {\n      logger.error('Failed to delete alert rule:', error);\n      throw error;\n    }\n  }\n  \n  /**\n   * Start the continuous evaluation loop\n   */\n  private startEvaluationLoop() {\n    // Evaluate rules every 30 seconds\n    setInterval(() => {\n      this.evaluateRules();\n    }, 30000);\n    \n    // Clean up old suppressions every 5 minutes\n    setInterval(() => {\n      this.cleanupSuppressions();\n    }, 5 * 60 * 1000);\n  }\n  \n  /**\n   * Evaluate all active rules against current performance data\n   */\n  private async evaluateRules() {\n    const performanceData = enhancedPerformanceMonitor.export();\n    const now = Date.now();\n    \n    for (const [ruleId, rule] of this.rules.entries()) {\n      if (!rule.enabled) continue;\n      \n      // Check if enough time has passed since last evaluation\n      const lastEvaluation = this.lastEvaluations.get(ruleId) || 0;\n      if (now - lastEvaluation < rule.evaluationWindow) continue;\n      \n      try {\n        await this.evaluateRule(rule, performanceData, now);\n        this.lastEvaluations.set(ruleId, now);\n      } catch (error) {\n        logger.error(`Failed to evaluate rule ${rule.name}:`, error);\n      }\n    }\n  }\n  \n  /**\n   * Evaluate a single rule\n   */\n  private async evaluateRule(rule: AlertRule, performanceData: any, timestamp: number) {\n    const value = this.extractMetricValue(rule.metric, performanceData);\n    if (value === null || value === undefined) return;\n    \n    const conditionMet = this.evaluateCondition(value, rule.operator, rule.threshold);\n    const alertKey = `${rule.id}_${rule.metric}`;\n    const existingAlert = this.activeAlerts.get(alertKey);\n    \n    if (conditionMet) {\n      if (!existingAlert) {\n        // Check suppression\n        if (this.suppressedAlerts.has(alertKey)) return;\n        \n        // Create new alert\n        await this.createAlert(rule, value, timestamp);\n        \n        // Add to suppression to prevent duplicate alerts\n        this.suppressedAlerts.add(alertKey);\n        setTimeout(() => {\n          this.suppressedAlerts.delete(alertKey);\n        }, rule.suppressionWindow);\n      }\n    } else if (existingAlert && rule.autoResolve) {\n      // Resolve existing alert if auto-resolve is enabled\n      const resolutionThreshold = rule.resolutionThreshold || rule.threshold;\n      if (this.evaluateCondition(value, this.getOppositeOperator(rule.operator), resolutionThreshold)) {\n        await this.resolveAlert(existingAlert.id, 'auto_resolved', { value });\n      }\n    }\n  }\n  \n  /**\n   * Extract metric value from performance data\n   */\n  private extractMetricValue(metric: string, data: any): number | null {\n    const parts = metric.split('.');\n    let value = data;\n    \n    for (const part of parts) {\n      if (value && typeof value === 'object' && part in value) {\n        value = value[part];\n      } else {\n        return null;\n      }\n    }\n    \n    return typeof value === 'number' ? value : null;\n  }\n  \n  /**\n   * Evaluate a condition\n   */\n  private evaluateCondition(value: number, operator: string, threshold: number): boolean {\n    switch (operator) {\n      case 'gt': return value > threshold;\n      case 'gte': return value >= threshold;\n      case 'lt': return value < threshold;\n      case 'lte': return value <= threshold;\n      case 'eq': return value === threshold;\n      case 'ne': return value !== threshold;\n      default: return false;\n    }\n  }\n  \n  /**\n   * Get opposite operator for auto-resolution\n   */\n  private getOppositeOperator(operator: string): string {\n    const opposites: Record<string, string> = {\n      'gt': 'lte',\n      'gte': 'lt',\n      'lt': 'gte',\n      'lte': 'gt',\n      'eq': 'ne',\n      'ne': 'eq'\n    };\n    return opposites[operator] || operator;\n  }\n  \n  /**\n   * Create a new alert\n   */\n  private async createAlert(rule: AlertRule, value: number, timestamp: number) {\n    const alert: PerformanceAlert = {\n      id: this.generateId(),\n      type: rule.metric,\n      severity: rule.severity,\n      message: this.generateAlertMessage(rule, value),\n      threshold: rule.threshold,\n      actualValue: value,\n      timestamp,\n      acknowledged: false,\n      metadata: {\n        ruleId: rule.id,\n        ruleName: rule.name,\n        category: rule.category,\n        metric: rule.metric,\n        tags: rule.tags\n      }\n    };\n    \n    const alertKey = `${rule.id}_${rule.metric}`;\n    this.activeAlerts.set(alertKey, alert);\n    \n    // Log alert event\n    await this.logAlertEvent({\n      id: this.generateId(),\n      ruleId: rule.id,\n      alertId: alert.id,\n      type: 'triggered',\n      timestamp: new Date().toISOString(),\n      value,\n      threshold: rule.threshold,\n      metadata: alert.metadata\n    });\n    \n    // Send notifications\n    await this.sendNotifications(alert, rule);\n    \n    // Schedule escalation if configured\n    if (rule.escalationRules && rule.escalationRules.length > 0) {\n      this.scheduleEscalation(alert, rule);\n    }\n    \n    logger.info(`Alert created: ${alert.message}`);\n  }\n  \n  /**\n   * Resolve an alert\n   */\n  async resolveAlert(alertId: string, reason: string, metadata?: Record<string, any>) {\n    const alertEntry = Array.from(this.activeAlerts.entries()).find(\n      ([, alert]) => alert.id === alertId\n    );\n    \n    if (!alertEntry) {\n      logger.warn(`Alert ${alertId} not found for resolution`);\n      return;\n    }\n    \n    const [alertKey, alert] = alertEntry;\n    alert.resolvedAt = Date.now();\n    \n    // Log resolution event\n    await this.logAlertEvent({\n      id: this.generateId(),\n      ruleId: alert.metadata?.ruleId,\n      alertId: alert.id,\n      type: 'resolved',\n      timestamp: new Date().toISOString(),\n      value: alert.actualValue,\n      threshold: alert.threshold,\n      reason,\n      metadata\n    });\n    \n    this.activeAlerts.delete(alertKey);\n    logger.info(`Alert resolved: ${alert.message} (${reason})`);\n  }\n  \n  /**\n   * Acknowledge an alert\n   */\n  async acknowledgeAlert(alertId: string, userId: string, notes?: string) {\n    const alertEntry = Array.from(this.activeAlerts.entries()).find(\n      ([, alert]) => alert.id === alertId\n    );\n    \n    if (!alertEntry) {\n      logger.warn(`Alert ${alertId} not found for acknowledgment`);\n      return;\n    }\n    \n    const [, alert] = alertEntry;\n    alert.acknowledged = true;\n    \n    // Log acknowledgment event\n    await this.logAlertEvent({\n      id: this.generateId(),\n      ruleId: alert.metadata?.ruleId,\n      alertId: alert.id,\n      type: 'acknowledged',\n      timestamp: new Date().toISOString(),\n      value: alert.actualValue,\n      threshold: alert.threshold,\n      userId,\n      notes\n    });\n    \n    logger.info(`Alert acknowledged by ${userId}: ${alert.message}`);\n  }\n  \n  /**\n   * Generate alert message\n   */\n  private generateAlertMessage(rule: AlertRule, value: number): string {\n    const metricName = rule.metric.split('.').pop() || rule.metric;\n    const operatorText = {\n      'gt': 'exceeded',\n      'gte': 'reached or exceeded',\n      'lt': 'dropped below',\n      'lte': 'reached or dropped below',\n      'eq': 'equals',\n      'ne': 'does not equal'\n    }[rule.operator] || rule.operator;\n    \n    return `${metricName} ${operatorText} threshold: ${value.toFixed(2)} (threshold: ${rule.threshold})`;\n  }\n  \n  /**\n   * Send notifications for an alert\n   */\n  private async sendNotifications(alert: PerformanceAlert, rule: AlertRule) {\n    for (const channel of rule.notificationChannels) {\n      if (!channel.enabled) continue;\n      \n      try {\n        await this.sendNotification(alert, rule, channel);\n      } catch (error) {\n        logger.error(`Failed to send notification via ${channel.type}:`, error);\n      }\n    }\n  }\n  \n  /**\n   * Send a single notification\n   */\n  private async sendNotification(alert: PerformanceAlert, rule: AlertRule, channel: NotificationChannel) {\n    const notification: AlertNotification = {\n      id: this.generateId(),\n      alertId: alert.id,\n      ruleId: rule.id,\n      channel,\n      status: 'pending',\n      retryCount: 0,\n      maxRetries: 3,\n      subject: `Performance Alert: ${rule.name}`,\n      message: this.formatNotificationMessage(alert, rule),\n      metadata: {\n        severity: alert.severity,\n        category: rule.category,\n        tags: rule.tags\n      }\n    };\n    \n    // Store notification record\n    await this.storeNotification(notification);\n    \n    // Send based on channel type\n    switch (channel.type) {\n      case 'email':\n        await this.sendEmailNotification(notification);\n        break;\n      case 'slack':\n        await this.sendSlackNotification(notification);\n        break;\n      case 'webhook':\n        await this.sendWebhookNotification(notification);\n        break;\n      case 'in_app':\n        await this.sendInAppNotification(notification);\n        break;\n      default:\n        logger.warn(`Unsupported notification channel: ${channel.type}`);\n    }\n  }\n  \n  /**\n   * Format notification message\n   */\n  private formatNotificationMessage(alert: PerformanceAlert, rule: AlertRule): string {\n    return `\n🚨 Performance Alert: ${rule.name}\n\nSeverity: ${alert.severity.toUpperCase()}\nMetric: ${rule.metric}\nCurrent Value: ${alert.actualValue.toFixed(2)}\nThreshold: ${alert.threshold}\nTime: ${new Date(alert.timestamp).toLocaleString()}\n\nDescription: ${rule.description}\n\nPlease investigate and take appropriate action.\n    `.trim();\n  }\n  \n  /**\n   * Schedule escalation for an alert\n   */\n  private scheduleEscalation(alert: PerformanceAlert, rule: AlertRule) {\n    rule.escalationRules?.forEach((escalationRule, index) => {\n      setTimeout(async () => {\n        // Check if alert is still active and not acknowledged\n        const currentAlert = Array.from(this.activeAlerts.values())\n          .find(a => a.id === alert.id);\n        \n        if (currentAlert && !currentAlert.acknowledged) {\n          // Check escalation condition if specified\n          if (escalationRule.condition) {\n            const performanceData = enhancedPerformanceMonitor.export();\n            const value = this.extractMetricValue(escalationRule.condition.metric, performanceData);\n            \n            if (value !== null) {\n              const conditionMet = this.evaluateCondition(\n                value,\n                escalationRule.condition.operator,\n                escalationRule.condition.threshold\n              );\n              \n              if (!conditionMet) return; // Don't escalate if condition not met\n            }\n          }\n          \n          // Escalate\n          await this.escalateAlert(alert, rule, escalationRule, index + 1);\n        }\n      }, escalationRule.delay);\n    });\n  }\n  \n  /**\n   * Escalate an alert\n   */\n  private async escalateAlert(alert: PerformanceAlert, rule: AlertRule, escalationRule: EscalationRule, level: number) {\n    // Update alert severity if escalation severity is higher\n    const severityOrder: Record<AlertSeverity, number> = {\n      'low': 1,\n      'medium': 2,\n      'high': 3,\n      'critical': 4\n    };\n    \n    if (severityOrder[escalationRule.severity] > severityOrder[alert.severity]) {\n      alert.severity = escalationRule.severity;\n    }\n    \n    // Log escalation event\n    await this.logAlertEvent({\n      id: this.generateId(),\n      ruleId: rule.id,\n      alertId: alert.id,\n      type: 'escalated',\n      timestamp: new Date().toISOString(),\n      value: alert.actualValue,\n      threshold: alert.threshold,\n      metadata: {\n        escalationLevel: level,\n        newSeverity: alert.severity\n      }\n    });\n    \n    // Send escalation notifications\n    for (const channel of escalationRule.channels) {\n      if (channel.enabled) {\n        try {\n          await this.sendNotification(alert, rule, channel);\n        } catch (error) {\n          logger.error(`Failed to send escalation notification:`, error);\n        }\n      }\n    }\n    \n    logger.info(`Alert escalated to level ${level}: ${alert.message}`);\n  }\n  \n  /**\n   * Store notification record\n   */\n  private async storeNotification(notification: AlertNotification) {\n    try {\n      const { error } = await supabase\n        .from('performance_alert_notifications')\n        .insert(notification);\n      \n      if (error) throw error;\n    } catch (error) {\n      logger.error('Failed to store notification record:', error);\n    }\n  }\n  \n  /**\n   * Log alert event\n   */\n  private async logAlertEvent(event: AlertEvent) {\n    try {\n      const { error } = await supabase\n        .from('performance_alert_events')\n        .insert(event);\n      \n      if (error) throw error;\n    } catch (error) {\n      logger.error('Failed to log alert event:', error);\n    }\n  }\n  \n  /**\n   * Send email notification\n   */\n  private async sendEmailNotification(notification: AlertNotification) {\n    // Implementation would integrate with email service (SendGrid, AWS SES, etc.)\n    logger.info(`Sending email notification to ${notification.channel.target}`);\n    \n    // Update notification status\n    notification.status = 'sent';\n    notification.sentAt = new Date().toISOString();\n    \n    // In a real implementation, you would:\n    // 1. Format the email with proper HTML/text\n    // 2. Send via your email service\n    // 3. Handle delivery confirmations\n    // 4. Implement retry logic for failures\n  }\n  \n  /**\n   * Send Slack notification\n   */\n  private async sendSlackNotification(notification: AlertNotification) {\n    // Implementation would integrate with Slack API\n    logger.info(`Sending Slack notification to ${notification.channel.target}`);\n    \n    notification.status = 'sent';\n    notification.sentAt = new Date().toISOString();\n  }\n  \n  /**\n   * Send webhook notification\n   */\n  private async sendWebhookNotification(notification: AlertNotification) {\n    try {\n      const response = await fetch(notification.channel.target, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n          ...(notification.channel.settings?.headers || {})\n        },\n        body: JSON.stringify({\n          alertId: notification.alertId,\n          ruleId: notification.ruleId,\n          subject: notification.subject,\n          message: notification.message,\n          severity: notification.metadata?.severity,\n          timestamp: new Date().toISOString()\n        })\n      });\n      \n      if (response.ok) {\n        notification.status = 'delivered';\n        notification.deliveredAt = new Date().toISOString();\n      } else {\n        notification.status = 'failed';\n        notification.failureReason = `HTTP ${response.status}: ${response.statusText}`;\n      }\n    } catch (error) {\n      notification.status = 'failed';\n      notification.failureReason = (error as Error).message;\n      logger.error('Webhook notification failed:', error);\n    }\n    \n    notification.sentAt = new Date().toISOString();\n  }\n  \n  /**\n   * Send in-app notification\n   */\n  private async sendInAppNotification(notification: AlertNotification) {\n    // Implementation would store in-app notification for user interface\n    logger.info(`Creating in-app notification for ${notification.channel.target}`);\n    \n    try {\n      const { error } = await supabase\n        .from('user_notifications')\n        .insert({\n          user_id: notification.channel.target,\n          type: 'performance_alert',\n          title: notification.subject,\n          message: notification.message,\n          severity: notification.metadata?.severity,\n          metadata: {\n            alertId: notification.alertId,\n            ruleId: notification.ruleId\n          },\n          read: false,\n          created_at: new Date().toISOString()\n        });\n      \n      if (error) throw error;\n      \n      notification.status = 'delivered';\n      notification.deliveredAt = new Date().toISOString();\n    } catch (error) {\n      notification.status = 'failed';\n      notification.failureReason = (error as Error).message;\n    }\n    \n    notification.sentAt = new Date().toISOString();\n  }\n  \n  /**\n   * Clean up old suppressions\n   */\n  private cleanupSuppressions() {\n    // Suppressions are automatically cleaned up by setTimeout\n    // This method could be extended to clean up persistent suppressions\n  }\n  \n  /**\n   * Get alert statistics\n   */\n  async getAlertStats(timeRange: { start: Date; end: Date }): Promise<AlertStats> {\n    try {\n      const { data: events, error } = await supabase\n        .from('performance_alert_events')\n        .select('*')\n        .gte('timestamp', timeRange.start.toISOString())\n        .lte('timestamp', timeRange.end.toISOString());\n      \n      if (error) throw error;\n      \n      const triggeredEvents = events?.filter(e => e.type === 'triggered') || [];\n      const resolvedEvents = events?.filter(e => e.type === 'resolved') || [];\n      const escalatedEvents = events?.filter(e => e.type === 'escalated') || [];\n      \n      // Calculate resolution times\n      const resolutionTimes: number[] = [];\n      triggeredEvents.forEach(triggered => {\n        const resolved = resolvedEvents.find(r => r.alertId === triggered.alertId);\n        if (resolved) {\n          const resolutionTime = new Date(resolved.timestamp).getTime() - new Date(triggered.timestamp).getTime();\n          resolutionTimes.push(resolutionTime);\n        }\n      });\n      \n      const averageResolutionTime = resolutionTimes.length > 0\n        ? resolutionTimes.reduce((sum, time) => sum + time, 0) / resolutionTimes.length\n        : 0;\n      \n      // Count by category and severity\n      const alertsByCategory: Record<string, number> = {};\n      const alertsBySeverity: Record<AlertSeverity, number> = {\n        low: 0,\n        medium: 0,\n        high: 0,\n        critical: 0\n      };\n      \n      triggeredEvents.forEach(event => {\n        const category = event.metadata?.category || 'unknown';\n        const severity = event.metadata?.severity || 'medium';\n        \n        alertsByCategory[category] = (alertsByCategory[category] || 0) + 1;\n        alertsBySeverity[severity as AlertSeverity]++;\n      });\n      \n      return {\n        totalAlerts: triggeredEvents.length,\n        activeAlerts: this.activeAlerts.size,\n        alertsByCategory,\n        alertsBySeverity,\n        averageResolutionTime,\n        falsePositiveRate: 0, // Would need additional logic to calculate\n        escalationRate: escalatedEvents.length / Math.max(triggeredEvents.length, 1) * 100,\n        topAlertRules: [] // Would need additional aggregation\n      };\n    } catch (error) {\n      logger.error('Failed to get alert stats:', error);\n      throw error;\n    }\n  }\n  \n  /**\n   * Get all rules\n   */\n  getRules(): AlertRule[] {\n    return Array.from(this.rules.values());\n  }\n  \n  /**\n   * Get active alerts\n   */\n  getActiveAlerts(): PerformanceAlert[] {\n    return Array.from(this.activeAlerts.values());\n  }\n  \n  /**\n   * Generate unique ID\n   */\n  private generateId(): string {\n    return `alert_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n}\n\n// =====================================================================================\n// DEFAULT ALERT RULES\n// =====================================================================================\n\nexport const DEFAULT_ALERT_RULES: Omit<AlertRule, 'id' | 'createdAt' | 'updatedAt' | 'createdBy'>[] = [\n  {\n    name: 'High API Response Time',\n    description: 'Alert when API response time exceeds 2 seconds',\n    category: 'performance',\n    metric: 'stats.api.average',\n    operator: 'gt',\n    threshold: 2000,\n    duration: 60000, // 1 minute\n    severity: 'high',\n    enabled: true,\n    autoResolve: true,\n    resolutionThreshold: 1000,\n    evaluationWindow: 30000, // 30 seconds\n    suppressionWindow: 300000, // 5 minutes\n    tags: ['api', 'response_time'],\n    notificationChannels: [\n      {\n        type: 'in_app',\n        target: 'admin',\n        enabled: true\n      }\n    ]\n  },\n  {\n    name: 'Critical Database Query Performance',\n    description: 'Alert when database queries take longer than 500ms',\n    category: 'performance',\n    metric: 'stats.query.p95',\n    operator: 'gt',\n    threshold: 500,\n    duration: 120000, // 2 minutes\n    severity: 'critical',\n    enabled: true,\n    autoResolve: true,\n    resolutionThreshold: 200,\n    evaluationWindow: 60000, // 1 minute\n    suppressionWindow: 600000, // 10 minutes\n    tags: ['database', 'query_performance'],\n    notificationChannels: [\n      {\n        type: 'in_app',\n        target: 'admin',\n        enabled: true\n      }\n    ],\n    escalationRules: [\n      {\n        delay: 300000, // 5 minutes\n        severity: 'critical',\n        channels: [\n          {\n            type: 'email',\n            target: 'admin@heypeter.academy',\n            enabled: true\n          }\n        ]\n      }\n    ]\n  },\n  {\n    name: 'Poor Core Web Vitals - LCP',\n    description: 'Alert when Largest Contentful Paint exceeds 4 seconds',\n    category: 'user_experience',\n    metric: 'detailedStats.webVitals.LCP.average',\n    operator: 'gt',\n    threshold: 4000,\n    duration: 180000, // 3 minutes\n    severity: 'medium',\n    enabled: true,\n    autoResolve: true,\n    resolutionThreshold: 2500,\n    evaluationWindow: 120000, // 2 minutes\n    suppressionWindow: 600000, // 10 minutes\n    tags: ['web_vitals', 'lcp', 'user_experience'],\n    notificationChannels: [\n      {\n        type: 'in_app',\n        target: 'admin',\n        enabled: true\n      }\n    ]\n  },\n  {\n    name: 'High Error Rate',\n    description: 'Alert when error rate exceeds 5%',\n    category: 'error_rate',\n    metric: 'stats.all.errorRate',\n    operator: 'gt',\n    threshold: 5,\n    duration: 60000, // 1 minute\n    severity: 'high',\n    enabled: true,\n    autoResolve: true,\n    resolutionThreshold: 2,\n    evaluationWindow: 30000, // 30 seconds\n    suppressionWindow: 300000, // 5 minutes\n    tags: ['errors', 'stability'],\n    notificationChannels: [\n      {\n        type: 'in_app',\n        target: 'admin',\n        enabled: true\n      }\n    ],\n    escalationRules: [\n      {\n        delay: 180000, // 3 minutes\n        severity: 'critical',\n        channels: [\n          {\n            type: 'email',\n            target: 'admin@heypeter.academy',\n            enabled: true\n          }\n        ]\n      }\n    ]\n  },\n  {\n    name: 'Bundle Size Too Large',\n    description: 'Alert when JavaScript bundle exceeds 2MB',\n    category: 'resource_usage',\n    metric: 'bundleMetrics.bundleSize',\n    operator: 'gt',\n    threshold: 2 * 1024 * 1024, // 2MB\n    duration: 300000, // 5 minutes\n    severity: 'medium',\n    enabled: true,\n    autoResolve: true,\n    resolutionThreshold: 1.5 * 1024 * 1024, // 1.5MB\n    evaluationWindow: 300000, // 5 minutes\n    suppressionWindow: 1800000, // 30 minutes\n    tags: ['bundle', 'performance', 'resources'],\n    notificationChannels: [\n      {\n        type: 'in_app',\n        target: 'admin',\n        enabled: true\n      }\n    ]\n  }\n];\n\n// =====================================================================================\n// SINGLETON INSTANCE\n// =====================================================================================\n\nexport const alertRuleEngine = new AlertRuleEngine();\n\n// Initialize default rules if none exist\nalertRuleEngine.getRules().length === 0 && setTimeout(async () => {\n  try {\n    for (const rule of DEFAULT_ALERT_RULES) {\n      await alertRuleEngine.addRule({\n        ...rule,\n        createdBy: 'system'\n      });\n    }\n    logger.info('Initialized default alert rules');\n  } catch (error) {\n    logger.error('Failed to initialize default alert rules:', error);\n  }\n}, 5000);"